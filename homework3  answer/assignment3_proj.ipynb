{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0111tJFWzICk"
      },
      "source": [
        "# Deep Learning Course\n",
        "\n",
        "## Assignment 3\n",
        "\n",
        "### Assignment Goals:\n",
        "\n",
        "* Implementing and improving RNN based language models.\n",
        "* Implementing and applying a Recurrent Neural Network on text classification problem.\n",
        "\n",
        "\n",
        "In this assignment, you will implement RNN-based language models and compare extracted word representation from different models. You will also compare two different training methods for sequential data: Truncated Backpropagation Through Time __(TBTT)__ and Backpropagation Through Time __(BTT)__.\n",
        "Also, you will be asked to apply Vanilla RNN to capture word representations and solve a text classification problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlK9zg28zICs"
      },
      "source": [
        "### DataSets\n",
        "\n",
        "You will use two datasets, an English Literature dataset for language model task (part 1 to 4) and the 20Newsgroups dataset for text classification (part 5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d6HhNVxzICs"
      },
      "source": [
        "### Requirements\n",
        "\n",
        "1. **(30 points) Implement a RNN based language model.**\n",
        "\n",
        "    Implement the RNN based language model described by [Mikolov et al.](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf), also called **Elman network**. The Elman network contains input, hidden and output layer and is trained by standard backpropagation (TBTT with $τ = 1$) using the cross-entropy loss.\n",
        "\n",
        "      * The input vector $x(t)$ at time $t$ consists of the current word while using 1-of-N coding (thus its size is equal to the size of the vocabulary) $w(t)$ and a vector $s(t − 1)$ which represents output values in the hidden layer from the previous time step $t-1$.\n",
        "      $$x(t) = w(t) + s(t-1)$$\n",
        "      \n",
        "      * The hidden layer is a fully connected tanh layer with size 500.\n",
        "      $$s_j(t) = f(\\sum_i x_i(t)u_{ji} )$$\n",
        "      Here $u$ is the parameter matrix of hidden layer, $f$ is the tanh activation function.\n",
        "      \n",
        "      * The softmax output layer captures a valid probability distribution.\n",
        "      $$y_k(t) = g(\\sum_j s_j(t)v_{kj})$$\n",
        "      Here $v$ is the parameter matrix of output layer, $g$ is the softmax function.\n",
        "      \n",
        "      * The model is trained with truncated backpropagation through time (TBTT) with $τ = 1$: the weights of the network are updated based on the error vector computed only for the current time step.\n",
        "      \n",
        "   Train the language model on the given English Literature dataset, report the model cross-entropy loss on the train set. Visualize the cross-entropy loss during training using a curve line. Your curve line should demonstrate that the loss value converges.\n",
        "   \n",
        "   Use nltk.word_tokenize to tokenize the documents. For initialization, $s(0)$ can be set to a vector of small values. Note that we are not interested in the *dynamic model* mentioned in the original paper.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1YWGQ_lzICt"
      },
      "source": [
        "2. **(20 points) Train the Elman network with BTT.**\n",
        "\n",
        "    TBTT has less computational cost and memory needs in comparison with **backpropagation through time algorithm (BTT)**. These benefits come at the cost of losing long term dependencies ([reference](https://arxiv.org/abs/1705.08209)). TBTT is rarely used until now, we use BTT instead.\n",
        "\n",
        "    Train your implemented Elman network with BTT, then compare the computational costs and performance of BTT and TBTT training.  For training the Elman-type RNN with BTT, one option is to perform mini-batch gradient descent with exactly one sentence per mini-batch. (Hints: The input  size will be (1, Sentence Length) ).\n",
        "\n",
        "   * Split the document into sentences (you can use nltk.tokenize.sent_tokenize. The natural language toolkit (nltk) can be installed using the command 'pip install nltk').\n",
        "   * For each sentence, perform one pass that computes the mean/sum loss for this sentence; then perform a gradient update for the whole sentence. (So the mini-batch size varies for the sentences with different lengths). You can truncate long sentences to fit the data in memory.\n",
        "   * Report the model cross-entropy loss.Visualize the cross-entropy loss during training using a curve line. Your curve line should demonstrate that the loss value converges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mieDS133zICu"
      },
      "source": [
        "3. **(30 points) Improve your Elman network with GRU.**\n",
        "\n",
        "    (a) Gated Recurrent Unit: It does not seem that simple recurrent neural networks can capture truly exploit context information with long dependencies, because of the problem of gradient vanishing and exploding. To solve this problem, gating mechanisms for recurrent neural networks were introduced. (15 points)\n",
        "\n",
        "    Try to learn your last model (Elman + BTT) with the SimpleRnn unit replaced with a **Gated Recurrent Unit (GRU)**. Report the model cross-entropy loss.  Visualize the cross-entropy loss during training using a curve line. Your curve line should demonstrate that the loss value converges. Compare your results in terms of cross-entropy loss with two other approaches (part 1 and 2).\n",
        "\n",
        "    (b) Text generation: Use each model to generate 10 synthetic sentences of 15 words each. Discuss the quality of the sentences generated - do they look like proper English? Do they match the training set? (15 points)\n",
        "    \n",
        "    Text generation from a given language model can be done using the following iterative process:\n",
        "   - Set sequence = \\[first_word\\], chosen randomly.\n",
        "   - Select a new word based on the sequence so far, add this word to the sequence, and repeat. At each iteration, select the word with maximum probability given the sequence so far. The trained language model outputs this probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW5DiR3CzICu"
      },
      "source": [
        "4. **(20 points) Implement a text classification model.**\n",
        "\n",
        "    We are aiming to learn an RNN model that predicts document categories given its content (text classification). For this task, we will use the 20Newsgroups dataset. The 20Newsgroupst contains messages from twenty newsgroups.  We selected four major categories (comp, politics, rec, and religion) comprising around 13k documents altogether. Your model should learn word representations to support the classification task. For solving this problem modify the **Elman network** architecture and simple RNN such that the last layer is a softmax layer with just 4 output neurons (one for each category).\n",
        "\n",
        "    * Download the 20Newsgroups dataset, and use the below helper function data_loader() to read in the dataset.\n",
        "    * Split the data into a training set (90%) and validation set (10%).\n",
        "    * Implement your text classification model, and train the model on 20Newsgroups training set.\n",
        "    * Report your accuracy results on the validation set. Try to achieve $\\geq 80\\%$ validation accuracy. (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAsw3u4ezICv"
      },
      "source": [
        "### Submission Notes\n",
        "\n",
        "Please use Jupyter Notebook. The notebook should include the final code, results and your answers. You should submit your Notebook in (.pdf or .html) and .ipynb format. (penalty 10 points)\n",
        "\n",
        "To reduce the parameters, you can merge all words that occur less often than a threshold into a special rare token (\\__unk__).\n",
        "\n",
        "**Instructions：**\n",
        "\n",
        "The university policy on academic dishonesty and plagiarism (cheating) will be taken very seriously in this course. Everything submitted should be your own writing or coding. You must not let other students copy your work. Spelling and grammar count.\n",
        "\n",
        "Your assignments will be marked based on correctness, originality (the implementations and ideas are from yourself), and test performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDUNZ8a6zICv"
      },
      "source": [
        "## Your Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9QxJU2fzICv"
      },
      "source": [
        "### 1. Implement a RNN based language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEkaZDw6zICv"
      },
      "source": [
        "### 2. Train the Elman network with BTT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq2rjOuizICw"
      },
      "source": [
        "### 3. Improve your Elman network with GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5IZyH62zICw"
      },
      "source": [
        "### 4. Implement a text classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1-ds2YMzICw"
      },
      "outputs": [],
      "source": [
        "\"\"\"This code is used to read all news and their labels\"\"\"\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def to_categories(name, cat=[\"politics\",\"rec\",\"comp\",\"religion\"]):\n",
        "    for i in range(len(cat)):\n",
        "        if str.find(name,cat[i])>-1:\n",
        "            return(i)\n",
        "    print(\"Unexpected folder: \" + name) # print the folder name which does not include expected categories\n",
        "    return(\"wth\")\n",
        "\n",
        "def data_loader(images_dir):\n",
        "    categories = os.listdir(data_path)\n",
        "    news = [] # news content\n",
        "    groups = [] # category which it belong to\n",
        "\n",
        "    for cat in categories:\n",
        "        print(\"Category:\"+cat)\n",
        "        for the_new_path in glob.glob(data_path + '/' + cat + '/*'):\n",
        "            news.append(open(the_new_path,encoding = \"ISO-8859-1\", mode ='r').read())\n",
        "            groups.append(cat)\n",
        "\n",
        "    return news, list(map(to_categories, groups))\n",
        "\n",
        "\n",
        "\n",
        "data_path = \"20Newsgroups_subsampled\"\n",
        "news, groups = data_loader(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY32RLu0zICy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}