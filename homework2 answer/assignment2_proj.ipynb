{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8380p9_VH5_P"
      },
      "source": [
        "# Deep Learning Course\n",
        "\n",
        "## Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNK1fADGH5_Q"
      },
      "source": [
        "### Assignment Goals\n",
        "\n",
        "* Design and implementation of CNNs.\n",
        "* CNN visualization.\n",
        "* Implementation of ResNet.\n",
        "\n",
        "In this assignment, you will be asked to learn CNN models for an image dataset. Different experiments will help you achieve a better understanding of CNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p12pwR7oH5_R"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset consists of around 9K images (some grayscale and some RGB) belonging to 101 classes. The shape of each image is (64,64,3). Every image is labeled with one of the classes. The image file is contained in the folder named after the class name.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqLJK2o2H5_R"
      },
      "source": [
        "### Requirements\n",
        "\n",
        "1. **(40 points) Implement and improve a CNN model.**\n",
        "\n",
        "   (a) We are aiming to learn a CNN on the given dataset. Download the dataset, and use PyTorch to implement LeNet5 to classify instances. Use a one-hot encoding for labels. The dataset is already split into training (90 percent) and validation (10 percent). Report the model loss (cross-entropy) and accuracy on both training and validation sets. (20 points)\n",
        "   \n",
        "    The LeNet5 configuration is:\n",
        "      - Convolutional layer (kernel size 5 x 5, 32 filters, stride 1 x 1 and followed by ReLU)\n",
        "      - Max Pooling layer with size 4 and stride 4 x 4\n",
        "      - Convolutional layer (kernel size 5 x 5, 64 filters, stride 1 x 1 and followed by ReLU)\n",
        "      - Max Pooling layer with size 4 and stride 4 x 4\n",
        "      - Fully Connected ReLU layer that has 1021 neurons\n",
        "      - Fully Connected ReLU layer with 84 neurons\n",
        "      - Fully Connected Softmax layer that has input 84 and output which is equal to the number of classes (one node for each of the classes).\n",
        "\n",
        "   (b) Try to improve model accuracy on the validation dataset by tuning the model hyperparameters. You can use any improvement methods you prefer. You are expected to reach at least 65 percent accuracy on validation set. (20 points)\n",
        "    \n",
        "    Here are some improvement methods you can use, of course you can use others which are not mentioned here:\n",
        "    - Dropout\n",
        "    - L1, L2 regularization\n",
        "    - Try improved initialization (e.g., Xavier initialier)\n",
        "    - Batch Normalization\n",
        "    \n",
        "   The grading of part (b) is based on the correctness of your implementation (5 points) and the performance of your improvement on the validation set. The validation accuracy and corresponding score is:\n",
        "    - 65% (5 points)\n",
        "    - 67% (8 points)\n",
        "    - 69% (12 points)\n",
        "    - 71% (15 points)\n",
        "\n",
        "\n",
        "   **Structure of LENET-5**\n",
        "   \n",
        "   This following LENET-5 structure is for 10-class dataset. Therefore, the layer size is not exactly the same as ours.\n",
        "   \n",
        "   ![hw2_lenet5.png](hw2_lenet5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5d_5SSAH5_S"
      },
      "source": [
        "2. **(20 points) Visualize layer activation**\n",
        "\n",
        "    There are several approaches to understand and visualize convolutional Networks, including visualizing the activations and layers weights. The most straight-forward visualization technique is to show the activations of the network during the forward pass. The second most common strategy is to visualize the weights. For more information we recommend the course notes on [\"Visualizing what ConvNets learn\"](http://cs231n.github.io/understanding-cnn/). More advanced techniques can be found in \"Visualizing and Understanding Convolutional Networks\" paper by Matthew D.Zeiler and Rob Fergus.\n",
        "    \n",
        "    Please visualize the layer activation of **the first conv layer** and **the second conv layer** of your above CNN model (after completing Q1), on the following 2 images:\n",
        "    - accordion/image_0001\n",
        "    - camera/image_0001\n",
        "    \n",
        "   Visualizing a CNN layer activation means to visualize the result of the activation layer as an image. Specifically, the activation of the first conv layer is the output of the first (conv + ReLU) layer during forward propagation. Since we have 32 filters in the first conv layer, you should draw 32 activation images for the first conv layer. Please display multiple images side by side in a row to make your output more readable (Hint: matplotlib.pyplot.subplot).\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ysIhJeH5_S"
      },
      "source": [
        "3. **(40 points) ResNet Implementation**\n",
        "\n",
        "    Use PyTorch to implement ResNet 18 to classify the given dataset. Same as above, please use a one-hot encoding for labels, split the dataset into training (90 percent) and validation (10 percent) and report the model loss (cross-entropy) and accuracy on both training and validation sets. See the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) for detailed introduction of ResNet.\n",
        "    \n",
        "    The grading of this part is mainly based on the implementation and performance on validation set. If you need more resources to complete the training, consider using Google Colab.\n",
        "    \n",
        "    The ResNet 18 configuration is:\n",
        "    -  conv_1 (kernel size 7 x 7, 64 filters, stride 2 x 2)\n",
        "    -  conv_2 (max pooling layer with size 3 x 3, followed by 2 blocks.Each block contains two conv layers. Each conv layer has kernel size 3 x 3, 64 filters, stride 2 x 2)\n",
        "    -  conv_3 (2 blocks, each contains 2 conv layers with kernel size 3*3, 128 filters)\n",
        "    -  conv_4 (2 blocks, each contains 2 conv layers with kernel size 3*3, 256 filters)\n",
        "    -  conv_5 (2 blocks, each contains 2 conv layers with kernel size 3*3, 512 filters)\n",
        "    \n",
        "   A block has the structure:\n",
        "   \n",
        "    ![hw2_resnet.png](hw2_resnet.png)\n",
        "    <!--<img src=\"attachment:image.png\" alt=\"drawing\" width=\"400\"/>-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhN-ZDNOH5_T"
      },
      "source": [
        "### Submission Notes\n",
        "\n",
        "Please use Jupyter Notebook. The notebook should include the final code, results and your answers. You should submit your Notebook in (.pdf or .html) and .ipynb format. (penalty 10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5UUtykGH5_T"
      },
      "source": [
        "## Your Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA1loKTqH5_T"
      },
      "outputs": [],
      "source": [
        "# You can use the following helper functions\n",
        "\n",
        "from typing import Any\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlT--sMWH5_U"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUOqrvPHH5_V"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ImageDataset( Dataset ):\n",
        "\n",
        "    def __init__(self, is_val= False, transform = None) -> None:\n",
        "\n",
        "\n",
        "        if is_val:\n",
        "            self.df = pd.read_csv( 'validation.csv', index_col=0 )\n",
        "        else:\n",
        "            self.df = pd.read_csv( 'train.csv', index_col= 0 )\n",
        "\n",
        "        self.cls_names = self.df['cls_name'].unique().tolist()\n",
        "        self.df['label'] = self.df['cls_name'].apply( self.cls_names.index )\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def get_num_classes(self):\n",
        "        return len( self.cls_names )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len( self.df )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.df.iloc[index]['path']\n",
        "        img = read_image( path ).type( torch.float32 )\n",
        "\n",
        "        target = self.df.iloc[index]['label']\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform( img )\n",
        "\n",
        "        target = torch.tensor( target )\n",
        "\n",
        "        return img/255 , target\n",
        "\n",
        "def collate_fn( batch ):\n",
        "    imgs, targets = [], []\n",
        "\n",
        "    for img, target in batch:\n",
        "        imgs.append( img )\n",
        "        targets.append( target )\n",
        "\n",
        "    imgs = torch.stack( imgs, dim= 0 )\n",
        "    targets = torch.stack( targets, dim= 0 )\n",
        "    return imgs, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQUWBQj6H5_V"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "learning_rate = 0.0001\n",
        "weight_decay = 0.001\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At0e6uWrH5_V"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "#     transforms.Normalize( (0.485, 0.456, 0.406), (0.229, 0.224, 0.225) ),\n",
        "    transforms.RandomVerticalFlip( .5 )\n",
        "])\n",
        "\n",
        "train_dataset = ImageDataset( is_val = False, transform = transform )\n",
        "val_dataset = ImageDataset( is_val = True )\n",
        "\n",
        "train_dataloader = DataLoader( train_dataset, batch_size = batch_size, shuffle= True, collate_fn=collate_fn )\n",
        "val_dataloader = DataLoader( val_dataset, batch_size = batch_size, shuffle= True, collate_fn=collate_fn )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BerMc9kH5_V"
      },
      "outputs": [],
      "source": [
        "\n",
        "def init_weights( m ):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwsUcZQYH5_V"
      },
      "source": [
        "###  Implement and improve a CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kRR5h6lH5_V"
      },
      "outputs": [],
      "source": [
        "# implement your Lenet5 here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knazG2PsH5_W"
      },
      "source": [
        "### Visualize layer activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6lPBFOSH5_X",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# implement your visualization here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54FZBNCRH5_Y"
      },
      "source": [
        "### ResNet Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL70C4NiH5_Y"
      },
      "outputs": [],
      "source": [
        "# implement a ResNet model here\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}